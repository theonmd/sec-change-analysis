{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5fd5baca-f927-4bbd-8515-e9733295f8db",
    "deepnote_cell_type": "text-cell-h1",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# SEC filings data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d0e6a349-e49d-403b-9be8-fd42ae4dd948",
    "deepnote_cell_height": 369,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3530,
    "execution_start": 1651078543409,
    "source_hash": "44bfdb44",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "%pip install w3lib\n",
    "from w3lib.html import remove_tags\n",
    "\n",
    "# Parsing and clenaing libraries\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import cleantext\n",
    "import nltk\n",
    "import inspect\n",
    "import toolz\n",
    "import tqdm\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0db33a65-6b12-44b3-a74d-ee44c3481c2b",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1d3adda7-c5bf-4146-a19c-cc620bc8abb2",
    "deepnote_cell_height": 153,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14,
    "execution_start": 1651078546948,
    "source_hash": "a93ee235",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url = \"https://www.sec.gov/Archives/edgar/data/748015/0001047469-11-000234.txt\"\n",
    "# real file: \"https://www.sec.gov/Archives/edgar/data/0000748015/000104746911000234/a2201619z10-k.htm\"\n",
    "\n",
    "# req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "# webpage = urlopen(req, timeout = 10).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "09eb33a9-6065-49e9-a011-49da3a299525",
    "deepnote_cell_height": 2259,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 39,
    "execution_start": 1651078546973,
    "source_hash": "685cbd16",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class files_cleaning():\n",
    "    '''\n",
    "    Class that takes as input a company code (stock format)\n",
    "    Creates a folder with all the clean 10-K files from this company\n",
    "    '''\n",
    "    def __init__(self, comp_code):\n",
    "        self.comp_code = comp_code\n",
    "\n",
    "    \n",
    "    def create_comp_dic(self):\n",
    "        '''\n",
    "        Function that creates a dictionnary with all the company's files\n",
    "        '''\n",
    "        comp_path = './data/{}/'.format(self.comp_code)\n",
    "        comp_raw_path = './data/{}/raw/'.format(self.comp_code)\n",
    "\n",
    "        if not os.path.isdir(comp_path):\n",
    "            raise Exception('The company has no valid folder!')\n",
    "\n",
    "        if not os.path.isdir(comp_raw_path):\n",
    "            raise Exception('The raw files folder is missing!')\n",
    "\n",
    "        comp_files = ['./data/{}/raw/'.format(self.comp_code) + f for f in listdir('./data/{}/raw'.format(self.comp_code))]\n",
    "        comp_files = [p for p in comp_files if p.split('/')[-1][0] != '.']\n",
    "        comp_dic = {}\n",
    "        for path in comp_files:\n",
    "            with open(path, 'r') as f:\n",
    "                year = path.split('_')[-1].replace('.txt', '')\n",
    "                text = f.read()\n",
    "                comp_dic[year] = text\n",
    "        return(comp_dic)\n",
    "\n",
    "    \n",
    "    def textual_content(self, file):\n",
    "        '''\n",
    "        Takes a 10-K file as input\n",
    "        Return the contextual information from the file\n",
    "        '''\n",
    "        # Define the regular expressions of the beginning / end of document\n",
    "        text_start_pattern = re.compile(r'<DOCUMENT>') \n",
    "        text_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "        type_pattern = re.compile(r'<TYPE>10-K[^\\n]+')\n",
    "\n",
    "        final_content = []\n",
    "\n",
    "        doc_start_list = [x.start() for x in text_start_pattern.finditer(file)] #assigns the first index from the starting pattern created before\n",
    "        doc_end_list = [x.end() for x in text_end_pattern.finditer(file)] #assigns the last index from the ending pattern created before\n",
    "        type_list = ['10-K']*len(doc_start_list) #assigns the type of the documents, which will always be 10-K's because we restricted it before\n",
    "\n",
    "        for doc_type, start_index, end_index in zip(type_list, doc_start_list, doc_end_list):\n",
    "            # Remove the last line? Looks to be useless in a few docs, TO CHECK\n",
    "            final_content.append(file[start_index:end_index])\n",
    "        return(' '.join(final_content))\n",
    "\n",
    "\n",
    "    def text_selection(self, html):\n",
    "        '''\n",
    "        Function that takes as input an html file\n",
    "        Returns the text extracted from that file\n",
    "        '''\n",
    "        notags = remove_tags(html)\n",
    "        soup = BeautifulSoup(notags,'html.parser')\n",
    "        return soup.get_text()\n",
    "\n",
    "\n",
    "    def clean_files(self):\n",
    "        '''\n",
    "        Function that takes as input the raw files\n",
    "        Return the clean files in a dictionnary\n",
    "        '''\n",
    "        comp_dic = self.create_comp_dic()\n",
    "\n",
    "        # Extract the textual content (isolate a portion of the html) \n",
    "        # print('Extracting relevant parts...')\n",
    "        # comp_dic_cont = toolz.valmap(self.textual_content, comp_dic)\n",
    "\n",
    "        # Extract the text\n",
    "        print('Extracting textual content...')\n",
    "        comp_dic_selec = {}\n",
    "        for year, html in comp_dic.items():\n",
    "            html_sel = self.text_selection(html)\n",
    "            comp_dic_selec[year] = html_sel\n",
    "\n",
    "        # comp_dic_selec = toolz.valmap(self.text_selection, comp_dic_cont)\n",
    "\n",
    "        # Clean the text (long to run...)\n",
    "        clean = lambda html: cleantext.clean(html, clean_all = True)\n",
    "        comp_dic_clean = {}\n",
    "        print('Cleaning the text (might take long)...')\n",
    "        for year, html in comp_dic_selec.items():\n",
    "            out_url = './data/{comp}/clean/{comp}_{year}_clean.txt'.format(comp=self.comp_code, year=year)\n",
    "\n",
    "            # Check if the clean file already exists or not\n",
    "            if not os.path.exists(out_url):\n",
    "                clean_html = clean(html)\n",
    "                comp_dic_clean[year] = clean_html\n",
    "\n",
    "        return(comp_dic_clean)\n",
    "\n",
    "    # Save the cleaned strings as txt files\n",
    "    def write_clean_files(self):\n",
    "        '''\n",
    "        Function that saves the clean strings from the original html into files\n",
    "        '''\n",
    "        comp_dic_clean = self.clean_files()\n",
    "        comp_clean_path = './data/{}/clean/'.format(self.comp_code)\n",
    "            \n",
    "        if not os.path.isdir(comp_clean_path):\n",
    "            os.makedirs(comp_clean_path)\n",
    "\n",
    "        print('Writing the clean files...')\n",
    "        for year, clean_string in comp_dic_clean.items():\n",
    "            # Output url\n",
    "            out_url = './data/{comp}/clean/{comp}_{year}_clean.txt'.format(comp=self.comp_code, year=year)\n",
    "\n",
    "            # Create the file\n",
    "            clean_file = open(out_url, 'wt')\n",
    "\n",
    "            # Remove non-utf8 characters\n",
    "            clean_string = ''.join(x for x in clean_string if x in string.printable)\n",
    "\n",
    "            # Write the file\n",
    "            n = clean_file.write(clean_string)\n",
    "            clean_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6a0b4d43-91d3-41e5-bf09-e0508405ebae",
    "deepnote_cell_height": 242.78125,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13,
    "execution_start": 1651078547697,
    "owner_user_id": "84094537-d0b3-4bb4-8a57-76f1d8ff63d1",
    "source_hash": "bcd06e53",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Apple object\n",
    "# apple_files = files_cleaning('AAPL')\n",
    "\n",
    "# Create the clean files\n",
    "# apple_files.write_clean_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "92d30a49-4af5-4e0c-8135-c71140da3b40",
    "deepnote_cell_height": 153,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1651078548858,
    "source_hash": "f3c2acbe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Tesla object\n",
    "# tsla_files = files_cleaning('TSLA')\n",
    "\n",
    "# Create the clean files\n",
    "# tsla_files.write_clean_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9356b43cb39342ab9372e3ca7cc03908",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "0815b927-3eb0-4ab3-b015-df0808a0b1d7",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
